<h1 align="center">ğŸ¤– Diabetes Prediction using Machine Learning</h1>

<p align="center">
  <em>A Binary Classification Project built on the <b>Pima Indians Diabetes Dataset</b></em><br>
  <strong>Predict diabetes risk instantly using Machine Learning âš¡</strong>
</p>

---

## ğŸŒŸ Overview

This project is a **binary classification** problem based on the popular *Pima Indians Diabetes Dataset*.  
It uses **Supervised Machine Learning algorithms** â€” *Logistic Regression, Random Forest, and XGBoost* â€”  
to predict whether a person is diabetic or not.

After evaluating all models, the **best-performing model** was deployed as an  
**interactive Streamlit web app** ğŸ§  allowing users to input health data and get predictions instantly.

---

## ğŸ”¥ Features

- âœ… End-to-end ML workflow (EDA â†’ Model Training â†’ Evaluation â†’ Deployment)  
- âœ… Multiple ML models trained and compared (*Logistic Regression, Random Forest, XGBoost*)  
- âœ… Accuracy, Precision, Recall, F1-Score comparison table  
- âœ… Feature scaling using **StandardScaler**  
- âœ… Model bundling using **joblib** for reusability  
- âœ… Interactive **Streamlit** web app with a clean UI  
- âœ… Instant diabetes risk prediction based on user input  

---

## ğŸ§© Tech Stack

| **Category** | **Tools Used** |
|---------------|----------------|
| ğŸ Language | Python |
| ğŸ¤– ML Libraries | scikit-learn, XGBoost, NumPy, Pandas |
| ğŸ“Š Visualization | Matplotlib, Seaborn |
| ğŸ’¾ Model Saving | joblib |
| ğŸŒ Web App Framework | Streamlit |
| ğŸ§  Environment | Kaggle Notebook + VS Code (Anaconda) |

---

## ğŸ“Š Model Development

<details>
<summary><b>1ï¸âƒ£ Data Understanding & Cleaning</b></summary>

- Used Pima Indians Diabetes Dataset from Kaggle  
- Checked null values, outliers, and feature distributions  
- Scaled features using `StandardScaler`
</details>

<details>
<summary><b>2ï¸âƒ£ Exploratory Data Analysis (EDA)</b></summary>

- Pairplots to visualize feature relationships  
- Observed that higher glucose and BMI correlated strongly with diabetes
</details>

<details>
<summary><b>3ï¸âƒ£ Model Training</b></summary>

- Trained and compared three algorithms:  
  - Logistic Regression  
  - Random Forest  
  - XGBoost  
</details>

---

## ğŸ”¢ Model Evaluation

| Model                | Accuracy | F1-Score | Precision | Recall |
|----------------------|-----------|-----------|------------|---------|
| ğŸ§© Logistic Regression  | **0.753** | **0.661** | **0.649** | **0.673** |
| ğŸŒ² Random Forest        | 0.734     | 0.631     | 0.625      | 0.636   |
| âš¡ XGBoost              | 0.708     | 0.615     | 0.581      | 0.654   |

ğŸ“ˆ **Best Model:** Logistic Regression âœ… *(Simple yet strong for this dataset)*

---

## ğŸ’¾ Model Saving

```python
obj = {"scaler": scaler, "model": model}
joblib.dump(obj, "scaler_model_bundle.joblib")
